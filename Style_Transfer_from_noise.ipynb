{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style Transfer from_noise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXcf2uKkkzyW",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "#Monetmaker\n",
        "\n",
        "@author: Leonardo Perugini, University of Bologna. https://github.com/leonardoperu\n",
        "\n",
        "\u003cbr/\u003e\n",
        "\n",
        "Project inspired by the paper \"\u003ca href\u003d\"https://arxiv.org/pdf/1508.06576.pdf\"\u003eA Neural Algorithm of Artistic style\u003c/a\u003e\" by L.A. Gatys, A.S. Ecker, M. Bethge (2015)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg_GbMBSlBPV",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "##Introduction\n",
        "####This application is built to transfer the artistic style of an image (e.g. a famous painting) on the content of a second picture:\n",
        "\n",
        "\u003e \u003ch3\u003e \u003cimg src\u003d\"https://drive.google.com/uc?export\u003dview\u0026id\u003d1GsBgdu2_ee5SUXlG4SJCyfd06oEZUOl_\" width \u003d\"500\" align\u003d\"center\"\u003e Basilica of San Petronio, Bologna (Italy)\u003c/h3\u003e\n",
        "\n",
        "\n",
        "\n",
        "\u003e \u003ch3\u003e \u003cimg src\u003d\"https://drive.google.com/uc?export\u003dview\u0026id\u003d1Qtnt0HTVdGp8YrPeJq7dyyLbtgX-8qML\" width \u003d\"500\" align\u003d\"center\"\u003e \"Twilight, Venice\" by Monet\u003c/h3\u003e\n",
        "\n",
        "\u003cbr/\u003e\u003cbr/\u003e\u003cbr/\u003e\n",
        "\n",
        "---\n",
        "\n",
        "\u003cbr/\u003e\u003cbr/\u003e\n",
        "####The result is obtained by adjusting the image in order to jointly minimize two loss functions (style loss and content loss), using a random noise image as the initial base:\n",
        "\n",
        "\u003e \u003ch3\u003e \u003cimg src\u003d\"https://drive.google.com/uc?export\u003dview\u0026id\u003d1-SVDs3GH1s_pDhZqWH6gGvBDsL9SrhjA\" width \u003d\"500\" align\u003d\"center\"\u003e Maybe, if Monet was from Bologna...\u003c/h3\u003e\n",
        "\n",
        "The initial image has been modified 8000 times (steps) to obtain this result.\n",
        "The code is presented below.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gmlbRuaK92Q",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Before starting, save a copy of this notebook in your Google Drive. Open it, go to `Edit \u003e Notebook Settings` and choose GPU under `Hardware acceleration`.\n",
        "\n",
        "Then, click on the folder icon (Files) on the left of the screen and click on \"Mount Drive\". This allows this program to load images from the folders inside `drive/My Drive/Colab Notebooks/imgs` (if they are not present, they are created by the next code cell)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVhJ5w-3GOOM",
        "colab_type": "code",
        "outputId": "427f7e1b-70b6-401f-dbb0-7f0c62ecc054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "pycharm": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.applications.vgg19 import VGG19, preprocess_input\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from urllib import request\n",
        "\n",
        "imgs_folder \u003d \"drive/My Drive/Colab Notebooks/imgs\"\n",
        "log_folder \u003d \"drive/My Drive/Colab Notebooks/Log/from_noise/\"\n",
        "Path(imgs_folder+\"/content\").mkdir(parents\u003dTrue, exist_ok\u003dTrue)\n",
        "Path(imgs_folder+\"/style\").mkdir(parents\u003dTrue, exist_ok\u003dTrue)\n",
        "Path(imgs_folder+\"/generated/transfer/from_noise/\").mkdir(parents\u003dTrue, exist_ok\u003dTrue)\n",
        "Path(log_folder).mkdir(parents\u003dTrue, exist_ok\u003dTrue)\n",
        "\n",
        "if not Path(imgs_folder+\"/content/san_luca.jpg\").exists():\n",
        "    request.urlretrieve(\"https://www.arcobalenoinviaggio.it/wp-content/uploads/2018/07/SAM_1328_small.jpg\",\n",
        "                        imgs_folder+\"/content/san_luca.jpg\")\n",
        "    \n",
        "if not Path(imgs_folder+\"/style/starry_night.jpeg\").exists():\n",
        "    request.urlretrieve(\"https://i.ebayimg.com/00/s/MTI3OVgxNTk5/z/L44AAOSwesVbRExS/$_3.JPG\",\n",
        "                        imgs_folder+\"/style/starry_night.jpg\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arOzyIYWED8x",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "##Utils\n",
        "Some utility functions needed by the application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh-ZUWeCGsuo",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": "def load_img(path, max_dim\u003d512):\n    \"\"\"\n    Used to load an image, described by its path, as a Tensor with values between 0 and 1.\n    The image is reshaped to make its longest dimension become equal to max_dim (by default 512)\n    \"\"\"\n    img \u003d tf.io.read_file(path)\n    img \u003d tf.image.decode_image(img, channels\u003d3)  # tf.Tensor, shape: (x, y, 3), int values in [0, 255]\n    img \u003d tf.image.convert_image_dtype(img, tf.float32) # tf.Tensor, shape: (x, y, 3), float values in [0, 1]\n    shape \u003d tf.cast(tf.shape(img)[:-1], tf.float32)\n    longest_dim \u003d max(shape)\n    scale \u003d max_dim / longest_dim\n    new_shape \u003d tf.cast(shape * scale, tf.int32)\n    img \u003d tf.image.resize(img, new_shape)\n    img \u003d img[tf.newaxis, :]    # shape (1, x, y, 3)\n    return img\n\n\ndef get_layers_by_name(model, layer_names):\n    \"\"\"\n    Used to obtain the list of the layers in the specified model, starting from their names\n    \"\"\"\n    return [model.get_layer(l) for l in layer_names]\n\n\ndef get_outputs_by_layer_names(model, layer_names):\n    \"\"\"\n    This returns the output features of the specified model layers\n    \"\"\"\n    result \u003d [model.get_layer(l).output for l in layer_names]\n    return result\n\n\ndef get_intermediate_layers_model(layer_names):\n    \"\"\"\n    Used to build a model based on the input and outputs of the specified layers, \n    starting from a VGG19 model without the fully connected layers\n    \"\"\"\n    vgg19 \u003d tf.keras.applications.VGG19(include_top\u003dFalse, weights\u003d\u0027imagenet\u0027, pooling\u003d\u0027avg\u0027)\n    vgg19.trainable \u003d False\n    output_layers \u003d get_outputs_by_layer_names(vgg19, layer_names)\n    intermediate_layers_model \u003d tf.keras.Model([vgg19.input], output_layers)\n    return intermediate_layers_model\n\n\ndef clip_image_0_1(img):\n    \"\"\"\n    Function to keep the (float) values of a processed image between 0 and 1\n    \"\"\"\n    return tf.clip_by_value(img, clip_value_min\u003d0.0, clip_value_max\u003d1.0)\n\ndef show_feature_maps(model, image):\n    \"\"\"\n    Function to show the feature maps in the layers of the passed model for the passed image\n    \"\"\"\n    feature_maps \u003d model.predict(image)\n    # plot the output from each block\n    square \u003d 8\n    for fmap in feature_maps:\n        # plot all 64 maps in an 8x8 squares\n        ix \u003d 1\n        for _ in range(square):\n            for _ in range(square):\n                # specify subplot and turn of axis\n                ax \u003d plt.subplot(square, square, ix)\n                ax.set_xticks([])\n                ax.set_yticks([])\n                # plot filter channel in grayscale\n                plt.imshow(fmap[:, :, ix-1], cmap\u003d\u0027gray\u0027)\n                ix +\u003d 1\n        # show the figure\n        plt.show()\n",
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebbk7r7eFXzg",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "###Logger\n",
        "This class helps to perform the logging part of the applicaton.\u003cbr/\u003e\n",
        "It can write to a specified file the used parameters, their changes and the loss values at the end of an execution step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhHxcERLGCsl",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "class Logger:\n",
        "    def __init__(self, log_folder, a, b, c, steps, content, style):\n",
        "        now \u003d time.localtime()\n",
        "        #self.log_path \u003d log_folder + \"a{:.1f}_b{:.2f}_c{}_{}_steps.txt\".format(a, b, c, steps)\n",
        "        self.log_path \u003d log_folder + style.split(\".\")[0] + \"_\" + content.split(\".\")[0] + \"_\" + time.strftime(\"%Y-%m-%d_%H-%M-%S.txt\", now)\n",
        "        self.logfile \u003d open(self.log_path, \"w\")\n",
        "        timestamp \u003d time.strftime(\"%Y-%m-%d %H:%M:%S\", now)\n",
        "        self.logfile.write(timestamp + \"\\n\")\n",
        "        self.logfile.write(\"Weights (content, style, variation) \u003d ({:.1f}, {:.2f}, {})\\n\".format(a, b, c))\n",
        "        self.logfile.write(\"Update steps: {}\\n\".format(steps))\n",
        "        self.logfile.write(\"Content image: \" + content + \"\\n\")\n",
        "        self.logfile.write(\"Style   image: \" + style + \"\\n\\n\")\n",
        "\n",
        "    def param_change(self, step, a, b, c, lr):\n",
        "        self.logfile.write(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
        "        self.logfile.write(\"Parameters changed at step {}: \".format(step) +\n",
        "                           \"weights (content, style, variation) \u003d ({:.1f}, {:.2f}, {}) ; \".format(a, b, c) +\n",
        "                           \"learning rate \u003d \" + str(lr) + \"\\n\")\n",
        "\n",
        "    def print_loss(self, step, loss, s_loss, c_loss):\n",
        "        self.logfile.write(\"Step {:\u003d5d} - total loss: {:\u003d16.1f}\\t[ S: {:\u003d14.1f} ; C: {:\u003d12.1f} ]\\n\".format(step, loss, s_loss, c_loss))\n",
        "        self.logfile.flush()\n",
        "\n",
        "    def close(self):\n",
        "        ts \u003d time.strftime(\"%H:%M:%S\", time.localtime())\n",
        "        self.logfile.write(\"\\nFinished at \" + ts + \"\\n\")\n",
        "        self.logfile.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH94gEJdOZaw",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "##Content extraction\n",
        "To reconstruct the content of the image, we use its feature representations in a chosen layer (here block4_conv2).\n",
        "\u003cbr/\u003e\n",
        "It is also possible to visualize these features (see the later instructions)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlcggVgfP2df",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "class ContentExtractor(tf.keras.models.Model):\n",
        "    \"\"\"\n",
        "    An instance of this class, when used to process an input image, \n",
        "    returns its content features extracted from the specified layer.\n",
        "    In this application, the chosen layer is  \"block4_conv2\".\n",
        "    \"\"\"\n",
        "    def __init__(self, content_layers):\n",
        "        super(ContentExtractor, self).__init__()\n",
        "        self.vgg \u003d get_intermediate_layers_model(content_layers) # vgg is a Model(input, output)\n",
        "        self.content_layers \u003d content_layers\n",
        "        self.vgg.trainable \u003d False\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # \"inputs\" is an image with float values between and 1\n",
        "        inputs \u003d inputs * 255\n",
        "        preprocessed_input \u003d preprocess_input(inputs)\n",
        "        content_outputs \u003d self.vgg(preprocessed_input)  # content features of the image\n",
        "        content_dict \u003d {self.content_layers[0]: content_outputs}\n",
        "        return content_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1DAKyYBQ6dI",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "The content loss functionis defined as the squared error between the feature representation of the processed image (content_outputs) and of the content image (content_target).\n",
        "\u003cbr/\u003e\n",
        "Thus, being x the generated image, p the original content image, F and P their\n",
        "respective feature representation and l the considered layer, the loss function is:\n",
        "\n",
        "$$L_{content}(p, x, l) \u003d w_c \\sum_{i, j}(F^l_{ij}-P^l_{ij})^2$$\n",
        "\n",
        "Since we use the content image itself as a base for the generated image, the value of this loss at the first iteration (step) will be 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7zITBus1-Nl",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "def content_loss(content_outputs, content_targets, weight\u003d0.5):\n",
        "    \"\"\"\n",
        "    Loss function to compute the distance between the content representation of\n",
        "    the processed image (content_outputs) and of the target style image (content_targets)\n",
        "    \"\"\"\n",
        "    loss \u003d weight * tf.add_n(\n",
        "        [tf.reduce_mean((content_outputs[l] - content_targets[l])**2) for l in content_outputs.keys()])\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFtMh358K0ux",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "##Style extraction\n",
        "As stated in the paper, it is possible to obtain a style representation of an image using the correlations between different convolutional filter responses.\n",
        "\u003cbr/\u003e\n",
        "These correlations are given by the Gram matrix of the feature maps in each layer considered, divided by the number.\n",
        "\u003cbr/\u003e\n",
        "The features are obtained as the output of the layers in the VGG19 model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WPbKARUNHh3",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def gram(x):\n",
        "    \"\"\"\n",
        "    Return a gram matrix for the given input matrix.\n",
        "    Args:\n",
        "        x: the matrix to calculate the gram matrix of\n",
        "    Returns:\n",
        "        the gram matrix of x\n",
        "    \"\"\"\n",
        "    shape \u003d K.shape(x[0])\n",
        "    # flatten the 3D tensor by converting each filter\u0027s 2D matrix of points\n",
        "    # to a vector, thus we have the matrix: [filter_width x filter_height, num_filters]\n",
        "    F \u003d K.reshape(x, (shape[0] * shape[1], shape[2]))\n",
        "    # take inner product over all the vectors to produce the Gram matrix\n",
        "    product \u003d K.dot(K.transpose(F), F)\n",
        "    product /\u003d tf.cast(shape[0]*shape[1], tf.float32)\n",
        "    shape \u003d K.shape([product])\n",
        "    return K.reshape(product, shape)\n",
        "\n",
        "\n",
        "class StyleExtractor(tf.keras.models.Model):\n",
        "    \"\"\"\n",
        "    An instance of this class, when used to process an input image, returns\n",
        "    its style features (Gram matrix) extracted from the specified layers.\n",
        "    In this application, the chosen layers are: \"block1_conv1\", \"block2_conv1\",\n",
        "    \"block3_conv1\", \"block4_conv1\", \"block5_conv1\".\n",
        "    \"\"\"\n",
        "    def __init__(self, style_layers):\n",
        "        super(StyleExtractor, self).__init__()\n",
        "        self.vgg \u003d get_intermediate_layers_model(style_layers) # vgg is a Model(input, output), not the function vgg_layers\n",
        "        self.style_layers \u003d style_layers\n",
        "        self.vgg.trainable \u003d False\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # \"inputs\" is an image with float values between and 1\n",
        "        inputs \u003d inputs * 255\n",
        "        preprocessed_input \u003d preprocess_input(inputs)\n",
        "        style_outputs \u003d self.vgg(preprocessed_input)  # style features of the image\n",
        "        style_outputs \u003d [gram(s) for s in style_outputs]\n",
        "        style_dict \u003d {layer: out for layer, out in zip(self.style_layers, style_outputs)}\n",
        "        return style_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9bis494NTKU",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "The style loss function is defined as the sum of the squared distances between the entries of the Gram matrix from the original image and the Gram matrix of the image to be generated, divided by the number of layers considered.\n",
        "\u003cbr/\u003e\u003cbr/\u003e\n",
        "Thus, being x and a the generated image and the original style image, A and G their respective style representation (Gram matrices) and L the set of the considered layers, the contribution of a layer to the style loss is:\n",
        "\n",
        "$$E_l \u003d \\frac{1}{N_l^2M_l^2} \\sum_{i, j}(G^l_{ij}-A^l_{ij})^2$$\n",
        "\n",
        "For each layer, the division by the number of filters N  and the feature maps dimension M (height times width) has already been computed in the gram(x) function.\n",
        "\u003cbr/\u003e\n",
        "\u003cbr/\u003e\n",
        "The total style loss is:\n",
        "$$L_{style}(a, x) \u003d \\sum^L_{l\u003d0}w_lE_l$$\n",
        "\u003cbr/\u003e\n",
        "In this case, the weight for each layer is equal to 1 divided by the number of layers in L.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXU1oGfOOORZ",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "def style_loss(style_outputs, style_targets, weight\u003d1.0):\n",
        "    \"\"\"\n",
        "    Loss function to compute the distance between the style representation (Gram matrix)\n",
        "    of the processed image (style_outputs) and of the target style image (style_targets)\n",
        "    \"\"\"\n",
        "    loss \u003d weight * tf.add_n(\n",
        "        [tf.reduce_mean((style_outputs[l] - style_targets[l]) ** 2) for l in style_outputs.keys()])\n",
        "    loss /\u003d float(len(style_outputs))\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT4XT0u5-f6K",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "##Style Transfer\n",
        "Here the content extractor and the style extractor are used together and their results are used to compute the total loss.\n",
        "\u003cbr/\u003e\n",
        "At each step, the generated image will be adjusted in order to minimize the value given by this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1JzYqzrAiEO",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "class StyleContentExtractor(tf.keras.models.Model):\n",
        "    \"\"\"\n",
        "    An instance of this class, when used to process an input image, returns\n",
        "    its style features (Gram matrix) and content features, extracted from the\n",
        "    specified layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, style_layers, content_layers):\n",
        "        super(StyleContentExtractor, self).__init__()\n",
        "        self.style_extractor \u003d StyleExtractor(style_layers)\n",
        "        self.content_extractor \u003d ContentExtractor(content_layers)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # \"inputs\" is an image with float values between and 1\n",
        "        inputs \u003d inputs * 255\n",
        "        preprocessed_input \u003d preprocess_input(inputs)\n",
        "        style_outputs \u003d self.style_extractor.vgg(preprocessed_input)  # style features of the image\n",
        "        style_outputs \u003d [gram(s) for s in style_outputs]\n",
        "        style_dict \u003d {layer: out for layer, out in zip(self.style_extractor.style_layers, style_outputs)}\n",
        "        content_outputs \u003d self.content_extractor.vgg(preprocessed_input)\n",
        "        content_dict \u003d {self.content_extractor.content_layers[0]: content_outputs}\n",
        "        return style_dict, content_dict\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7KdrlbDAlFf",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Being a the artwork (style image) and p the photograph (content image), the total loss function we want to minimize is given by:\n",
        "\u003cbr/\u003e\u003cbr/\u003e\n",
        "$$L_{total}(p, a, x) \u003d \\alpha L_{content}(p, x) + \\beta L_{style}(a, x)$$\n",
        "\u003cbr/\u003e\n",
        "Alpha and beta are the weighing factors for content and style reconstruction respectively.\n",
        "\u003cbr/\u003e\n",
        "After many experiments, the chosen values were 40.0 for alpha and 0.8 for beta. In the later steps of the execution, beta may increase slightly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mUUUvHmDG4g",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "def total_loss(style_outputs, style_targets, content_outputs, content_targets, weights):\n",
        "    \"\"\"\n",
        "    Loss function to compute the total loss from the generated image and the couple of target images\n",
        "    \"\"\"\n",
        "    c_loss \u003d content_loss(content_outputs, content_targets, weights[0])\n",
        "    s_loss \u003d style_loss(style_outputs, style_targets, weights[1])\n",
        "    loss \u003d s_loss + c_loss\n",
        "    return loss, s_loss, c_loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Janr5jiKDJb4",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "The updating of the generated image is performed using `tf.GradientTape()`.\u003cbr/\u003e\n",
        "An optimizer is needed to apply the computed gradients. This function is executed once per step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsmHdVbAG_2e",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "total_variation_weight \u003d 40\n",
        "def update_image_step(gen_image, style_target, content_target, optimizer, extractor, weights):\n",
        "    with tf.GradientTape() as tape:\n",
        "        gen_style_output, gen_content_output \u003d extractor(gen_image)\n",
        "        loss, s_loss, c_loss \u003d total_loss(gen_style_output, style_target, gen_content_output, content_target, weights)\n",
        "        loss +\u003d total_variation_weight * tf.image.total_variation(gen_image)\n",
        "    grad \u003d tape.gradient(loss, gen_image)\n",
        "    optimizer.apply_gradients([(grad, gen_image)])\n",
        "    gen_image.assign(clip_image_0_1(gen_image))\n",
        "    return loss, s_loss, c_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVdpdhqbHF69",
        "colab_type": "text",
        "pycharm": {}
      },
      "source": [
        "Finally, here are the instructions to run the entire program and actually execute the artistic style transfer algorithm.\n",
        "\n",
        "The optimization is performed for 8000 `steps`. The statistics and the current generated image are printed every 200 (`print_step`) steps.\n",
        "\n",
        "To visualize the content features of the initial content image in the model layers, set `features_visualization` to `True`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIN-SUs-JdZs",
        "colab_type": "code",
        "colab": {},
        "pycharm": {}
      },
      "source": [
        "features_visualization \u003d False\n",
        "\n",
        "style_name \u003d \"poppies.jpg\"\n",
        "content_name \u003d \"canoscio1.png\"\n",
        "\n",
        "steps \u003d 8000\n",
        "print_step \u003d 200\n",
        "\n",
        "W \u003d (1200, 0.8) # content weight, style weight\n",
        "\n",
        "style_path \u003d imgs_folder + \"/style/\" + style_name\n",
        "content_path \u003d imgs_folder + \"/content/\" + content_name\n",
        "style_layers \u003d [\"block1_conv1\", \"block2_conv1\", \"block3_conv1\", \"block4_conv1\", \"block5_conv1\"]\n",
        "# style_layers \u003d [\"block2_conv1\", \"block3_conv1\", \"block4_conv1\", \"block5_conv1\"]\n",
        "content_layers \u003d [\"block4_conv2\"]\n",
        "\n",
        "logger \u003d Logger(log_folder, W[0], W[1], total_variation_weight, steps, content_name, style_name)\n",
        "\n",
        "content_image \u003d load_img(content_path)  # tf.Tensor, shape (1, x\u0027, y\u0027, 3), values in [0, 1]\n",
        "style_image \u003d load_img(style_path)      # the longest dimension x\u0027 or y\u0027 is equal to 512\n",
        "\n",
        "\"\"\"It is possible to start the generation from the content image rather than\n",
        "   from a noise image by commenting these two lines and uncommenting the next one.\n",
        "   If you do so, set the content weight to 40.0 (lines 9 and 46) \"\"\"\n",
        "white_noise \u003d np.random.uniform(0, 1, content_image.shape)\n",
        "gen_image \u003d tf.Variable(white_noise, dtype\u003dtf.float32)\n",
        "# gen_image \u003d tf.Variable(content_image, dtype\u003dtf.float32)\n",
        "\n",
        "# plt.imshow(np.array(gen_image[0]))\n",
        "# plt.show()\n",
        "\n",
        "extractor \u003d StyleContentExtractor(style_layers, content_layers)\n",
        "style_targets, _ \u003d extractor(style_image)\n",
        "_, content_targets \u003d extractor(content_image)\n",
        "\n",
        "if features_visualization:\n",
        "    show_feature_maps(extractor.content_extractor.vgg, content_image)\n",
        "\n",
        "opt \u003d tf.keras.optimizers.Adam(learning_rate\u003d0.05)\n",
        "print()\n",
        "\n",
        "loss, s_loss, c_loss \u003d 0, 0, 0\n",
        "save_as \u003d imgs_folder + \"/generated/transfer/from_noise/\" + style_name.split(\".\")[0] + \"_\"\n",
        "save_as +\u003d content_name.split(\".\")[0] + \"_\" + str(steps) + \"_steps_new.jpg\"\n",
        "\n",
        "half \u003d { 500: (1200, 0.8), 3000: (1200, 1.0) }\n",
        "\n",
        "for _ in tqdm(range(steps), file\u003dsys.stdout):\n",
        "    if _ \u003d\u003d 1 or _ !\u003d 0 and _ % print_step \u003d\u003d 0:\n",
        "        print(\"\\tLoss at step {}:\\t{}\\t[ S: {:.2f}; C: {:.2f} ]\".format(_, loss, s_loss, c_loss))\n",
        "        logger.print_loss(_, loss[0], s_loss, c_loss)\n",
        "        plt.imshow(np.array(gen_image[0]))\n",
        "        plt.show()\n",
        "    if _ \u003d\u003d 200:\n",
        "        opt.learning_rate \u003d 0.04\n",
        "        logger.param_change(_, W[0], W[1], total_variation_weight, 0.04)\n",
        "    if _ in half.keys() :\n",
        "        opt.learning_rate.assign(opt.learning_rate/2)\n",
        "        W \u003d half[_]\n",
        "        print(\"\u003d\"*20 + \"new weights: \" + str(half[_]) + \", lr halved\" + \"\u003d\"*20)\n",
        "        logger.param_change(_, W[0], W[1], total_variation_weight, opt.learning_rate.numpy())\n",
        "    loss, s_loss, c_loss \u003d update_image_step(gen_image, style_targets, content_targets, opt, extractor, W)\n",
        "\n",
        "print(\"\\t\"*12 + \"Loss at step {}:\\t{}\\t[ S: {:.2f}; C: {:.2f} ]\".format(steps, loss, s_loss, c_loss))\n",
        "logger.print_loss(_+1, loss[0], s_loss, c_loss)\n",
        "logger.close()\n",
        "plt.imshow(np.array(gen_image[0]))\n",
        "plt.show()\n",
        "\n",
        "array \u003d np.array((gen_image[0] * 255), dtype\u003dnp.uint8)\n",
        "Image.fromarray(array).save(save_as)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}